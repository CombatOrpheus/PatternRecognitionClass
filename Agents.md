# Project Agents and Workflow

This document outlines the automated agents (scripts) that constitute the MLOps pipeline for this project. These agents are designed to be run sequentially to handle hyperparameter optimization, model training, cross-validation, and results analysis in a structured and reproducible manner.

## The ML Workflow

The project follows a standard yet robust machine learning workflow, orchestrated by a series of command-line scripts. The main entry point is `scripts/main.py`, which runs the entire pipeline. The typical order of operations is as follows:

1.  **Hyperparameter Optimization**: Find the best model architecture and training parameters.
2.  **Model Training**: Perform multiple runs with the best parameters to ensure statistical significance.
3.  **Cross-Validation**: Evaluate the trained models on a variety of unseen datasets.
4.  **Analysis and Plotting**: Aggregate the results from training and validation to generate insights.

## Project Configuration and Setup

This project uses `uv` for fast dependency management, as defined in the `pyproject.toml` file.

### Environment Setup

To create a virtual environment and install all necessary packages, run the following commands from the root of the project:

1.  **Create and activate the virtual environment**:
    ```bash
    # Install uv if you don't have it
    pip install uv

    # Create and activate the virtual environment
    uv venv
    source .venv/bin/activate
    ```

2.  **Install dependencies from `pyproject.toml`**:
    ```bash
    uv pip sync pyproject.toml
    ```
This ensures you have a reproducible environment with the exact versions of the libraries used for development.

### Coding Style and Conventions

To maintain code quality and readability, this project adheres to the following conventions:

*   **Code Formatting**: All Python code is formatted using the **Black** code formatter.
*   **Type Hinting**: All functions and methods should include type hints.
*   **Docstrings**: All modules, classes, and functions must have clear and concise docstrings. This project follows the **Google Style Python Docstrings**.
*   **Modularity**: The project is structured into distinct modules (`src`, `scripts`, `Data`) with clear responsibilities.

---

## Agent 0: Main Pipeline Orchestrator

### `scripts/main.py`

*   **Purpose**: This is the main entry point for the entire MLOps workflow. It sequentially runs the optimization, training, and analysis scripts.
*   **Usage**: `python scripts/main.py`

---

## Agent 1: Hyperparameter Optimization

There are two specialized agents for this task, depending on the model type.

### `scripts/optimize_hyperparameters.py` (for Homogeneous Models)

*   **Purpose**: Uses **Optuna** to perform an automated search for the best hyperparameters for the homogeneous GNN models (GCN, TAG, etc.).
*   **Outputs**: An SQLite database file for each GNN operator, containing the results of all trials.
*   **Usage**: `python scripts/optimize_hyperparameters.py`

### `scripts/optimize_hyperparameters_hetero.py` (for Heterogeneous Models)

*   **Purpose**: Performs the same function as its homogeneous counterpart but is tailored for heterogeneous models like RGAT and HEAT.
*   **Outputs**: An SQLite database file for each GNN operator.
*   **Usage**: `python scripts/optimize_hyperparameters_hetero.py`

---

## Agent 2: Model Training

### `scripts/train_model.py`

*   **Purpose**: After identifying the best hyperparameters, this agent performs multiple training runs (`--num_runs`) using different random seeds to validate the statistical stability of the model's performance.
*   **Inputs**: The Optuna study databases created by the optimization agents.
*   **Outputs**:
    *   Model checkpoints (`.ckpt`) for each run.
    *   A `statistical_results.parquet` file summarizing performance metrics.
*   **Usage**: `python scripts/train_model.py`

---

## Agent 3: Cross-Validation

### `scripts/test_model.py`

*   **Purpose**: Evaluates previously trained models against a collection of new or different datasets without needing to retrain.
*   **Inputs**: A directory containing saved model checkpoints.
*   **Outputs**: A Parquet file containing the performance metrics of each model on each dataset.
*   **Usage**: `python scripts/test_model.py`

---

## Agent 4: Analysis and Plotting

### `src/Analysis.py`

*   **Purpose**: This is the final agent in the pipeline. It consumes the Parquet files generated by the training and cross-validation agents to produce a comprehensive analysis, including statistical tests, summary tables, and visualizations.
*   **Inputs**: The Parquet files from the training and cross-validation steps.
*   **Outputs**: A directory containing CSV summaries and plot files (`.svg`).
*   **Usage**: This script is called by `scripts/main.py`.
