{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f581f6f-639f-444c-8483-935357a15bde",
   "metadata": {},
   "source": [
    "# Parsing the Data\n",
    "The data set is available in a JSON format, in both original and pre-processed formats. Since my needs are different from that of the original authors, I'll be using the original data. The data is organised as follows:\n",
    "- `data{index}`\n",
    "  - **petri_net**: A compound matrix $[I; O; M_0]$ representing the Petri Net; for a Petri Net with $N$ transitions, the compound matrix will have a length of $N*2 + 1$.\n",
    "  - **arr_vlist**: The reachable markings of this Petri Net.\n",
    "  - **arr_edge**: The edges (source, destination) for the reachable markings.\n",
    "  - **arr_tranidx**: The transition that fired, leading to the creation of a new marking.\n",
    "  - **spn_labda**: The $\\lambda$ (average firing rate) of the transition.\n",
    "  - **spn_steadyprob**: The steady probability distribution of the Petri Net.\n",
    "  - **spn_markdens**: The token probability density function of the Petri Net.\n",
    "  - **spn_mu**: The average number of tokens in the network in the steady state.\n",
    "\n",
    "For my purposes, only the first four data points are of interest, so I'll focus on extracting them. These correspond to the *Petri Net* and the *Reachability Graph*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c1e0c-9182-42a6-b9c8-dffca5bba224",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Due to the size of the files and the number of examples, it is more efficient to iterate through the values and build individual values. Python's standard JSON module is not adequate for this because it has to read the entire file before parsing, which is wasteful. Instead, `ijson` offers a steaming alternative, which is what I will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd6c1227-5abd-4d3f-a5e8-06d2ba7dbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_stream\n",
    "import numpy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a29d6a-57fe-4f5f-a7cb-0f766844ac9e",
   "metadata": {},
   "source": [
    "### Typing Information\n",
    "Since I don't like to guess the return of a function based on its name, typing information is going to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4e8ce4-0603-4362-918b-ff270b74ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0c8bc-c0de-47e4-93a5-b8815e104ca6",
   "metadata": {},
   "source": [
    "# Reading the Data\n",
    "The data is already arranged in a readable format, which means that we simply have to parse the JSON structure.\n",
    "## The First Solution: Default Python Module\n",
    "Python offers a JSON decoder/encoder in its standard library, and it might prove sufficient for our needs. The only issue with it, at first, is that it loads the entire file into memory before reading, which might slow down the process considerably for some of the larger files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e42f3d-fb8e-4401-b12d-7ba37ce26b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict_blocking(file: Path) -> Dict:\n",
    "    with open(file) as source:\n",
    "        return json.load(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d03c6-bffa-4bdc-903f-09c4bd8d1974",
   "metadata": {},
   "source": [
    "### Partial Solution: Asynchronous Reading\n",
    "Python's `asyncio` module offers high-level operations for asynchronous operations, particularly geared towards IO, which is exactly what I am doing. This does not solve the entire file still needs to be loaded into memory and then parsed, which is still a slow operation, considering that no other concurrent work is going to happen at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aba9f41-b79d-45af-a7c6-2a394e0df48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_data_dic_async(file: Path) -> Dict:\n",
    "    with open(file) as source:\n",
    "        return json.load(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35b089-b276-4850-8a73-110a04ff8f87",
   "metadata": {},
   "source": [
    "## A Second Attempt: Chunked Reading\n",
    "Since blocking the entire process while reading and decoding the file is not ideal, iterating over the data, decoding item by item, and converting them concurrently should provide a reasonable performance improvement.\n",
    "\n",
    "The `json_stream` library offers this feature transparently, although we do need to pay attention due to its iterator-based nature. We can only read the file once, and we need to store the results to use them. This is not too problematic for us since multiple conversions are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d4ef52-1021-43a1-910c-a3056aaa9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict_stream(file: Path) -> Dict:\n",
    "    with open(file) as source:\n",
    "        return json_stream.load(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00616ff7-2aa5-4cb9-9491-5e53446b170b",
   "metadata": {},
   "source": [
    "# Converting into Numpy Arrays/Tensors\n",
    "The data is read as a bunch of Python lists, which is not adequate for feeding into Neural Networks. For that end, I have to provide functions to convert this data into Numpy arrays, which are then trivially converted into PyTorch/TensorFlow Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947c377f-7cf1-4f52-ba39-82156405e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_petri_nets(file: Path) -> Iterable:\n",
    "    data_dictionary = get_data_dict_blocking(file)\n",
    "    for key in data_dictionary.keys():\n",
    "        data = data_dictionary[key]\n",
    "        pn = numpy.array(data['petri_net'])\n",
    "        reachable_markings = numpy.array(data['arr_vlist'])\n",
    "        edges = numpy.array(data['arr_edge'])\n",
    "        fired_transitions = numpy.array(data['arr_tranidx'])\n",
    "        yield (pn, reachable_markings, edges, fired_transitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
