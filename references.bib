@Comment{
ebib-main-file: /home/gsoliveira/Sync/10-19 Personal/14 Knowledge Management Systems/14.02 JabRef/Citar References.bib
}


@InProceedings{10.1007/3-540-46429-8_30,
	author = "Hirel, Christophe
and Tuffin, Bruno
and Trivedi, Kishor S.",
	editor = "Haverkort, Boudewijn R.
and Bohnenkamp, Henrik C.
and Smith, Connie U.",
	title = "SPNP: Stochastic Petri Nets. Version 6.0",
	booktitle = "Computer Performance Evaluation.Modelling Techniques and Tools",
	year = "2000",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "354--357",
	abstract = "The Stochastic Petri Net Package (SPNP) [2] is a versatile modeling tool for solution of Stochastic Petri Net (SPN) models. The SPN models are described in the input language for SPNP called CSPL (C-based SPN Language) which is an extension of the C programming language [8] with additional constructs which facilitate easy description of SPN models. Moreover, if the user does not want to describe his model in CSPL, a Graphical User Interface (GUI) is available to specify all the characteristics as well as the parameters of the solution method chosen to solve the model.",
	isbn = "978-3-540-46429-7"
}

@InProceedings{10.1007/978-3-319-66335-7_19,
	author = "Zimmermann, Armin",
	editor = "Bertrand, Nathalie
and Bortolussi, Luca",
	title = "Modelling and Performance Evaluation with TimeNET 4.4",
	booktitle = "Quantitative Evaluation of Systems",
	year = "2017",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "300--303",
	abstract = "The paper presents the current status of the software tool TimeNET. It supports modeling and performance evaluation of stochastic models, including extended deterministic and stochastic Petri nets, colored stochastic Petri nets, and Markov chains as well as UML extensions. Among its main characteristics are simulation and analysis modules for stationary and transient evaluation of Petri nets including non-exponentially distributed delays, as well as a simulation module for complex colored models. Recent enhancements include algorithms for the efficient rare-event simulation of Petri nets, a new multi-trajectory hybrid simulation/analysis algorithm, and a net class for Markov chains.",
	isbn = "978-3-319-66335-7"
}

@Collection{AjmoneMarsan1995,
	date = {1995},
	editor = {Ajmone Marsan, Marco},
	title = {Modelling with generalized stochastic Petri nets},
	isbn = {978-0-471-93059-4},
	location = {Chichester},
	pagetotal = {301},
	publisher = {Wiley},
	series = {Wiley series in parallel computing},
	address = {Chichester},
	year = {1995}
}

@Article{Drakaki2016,
	author = {Drakaki, Maria and Tzionas, Panagiotis},
	date = {2016-07},
	journaltitle = {International Journal of Computer Integrated Manufacturing},
	title = {Modeling and performance evaluation of an agent-based warehouse dynamic resource allocation using {Colored} {Petri} {Nets}},
	doi = {10.1080/0951192X.2015.1130239},
	issn = {0951-192X, 1362-3052},
	language = {en},
	number = {7},
	pages = {736--753},
	urldate = {2022-12-26},
	volume = {29}
}

@Article{Drakaki2017,
	author = {Drakaki, Maria and Tzionas, Panagiotis},
	date = {2017},
	journaltitle = {Applied Sciences},
	title = {Manufacturing {Scheduling} {Using} {Colored} {Petri} {Nets} and {Reinforcement} {Learning}},
	doi = {10.3390/app7020136},
	issn = {2076-3417},
	number = {2},
	url = {https://www.mdpi.com/2076-3417/7/2/136},
	volume = {7},
	abstract = {Agent-based intelligent manufacturing control systems are capable to efficiently respond and adapt to environmental changes. Manufacturing system adaptation and evolution can be addressed with learning mechanisms that increase the intelligence of agents. In this paper a manufacturing scheduling method is presented based on Timed Colored Petri Nets (CTPNs) and reinforcement learning (RL). CTPNs model the manufacturing system and implement the scheduling. In the search for an optimal solution a scheduling agent uses RL and in particular the Q-learning algorithm. A warehouse order-picking scheduling is presented as a case study to illustrate the method. The proposed scheduling method is compared to existing methods. Simulation and state space results are used to evaluate performance and identify system properties.},
	priority = {prio2}
}

@inproceedings{Fey/Lenssen/2019,
	title = {Fast Graph Representation Learning with {PyTorch Geometric}},
	author = {Fey, Matthias and Lenssen, Jan E.},
	booktitle = {ICLR Workshop on Representation Learning on Graphs and Manifolds},
	year = {2019}
}

@InProceedings{Guang2021,
	author = {Guang, Mingjian and Yan, Chungang and Wang, Junli and Qi, Hongda and Jiang, Changjun},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	date = {2021-07},
	title = {Benchmark {Datasets} for {Stochastic} {Petri} {Net} {Learning}},
	doi = {10.1109/IJCNN52387.2021.9533785},
	note = {ISSN: 2161-4407},
	pages = {1--8},
	abstract = {The existing Stochastic Petri Net (SPN) analysis methods are based on a series of steps, including generating the reachable graph, solving the state equation, etc. Unfortunately, these methods cannot perform performance analysis when the state equation has no unique solution. The end-to-end deep learning methods can build a mapping relationship from SPN to performance indicators, which avoids solving the state equation. However, there is a lack of benchmark datasets for SPN learning and training. This paper proposes an automatic generation method of SPN datasets, including SPN random generation, data labeling, data enhancement, and filtering. To relieve the local aggregation problem of random-based organization, a grid-based data organization method is proposed to ensure the diversity of the datasets. In the experimental section, the generated datasets are trained and tested on three types of neural networks. The results show that the generated benchmark datasets are useful, and the increase of dataset size will significantly improve the learning performance.}
}

@Article{Lin2020,
	author = {Lin, Yi-Nan and Hsieh, Tsang-Yen and Yang, Cheng-Ying and Shen, Victor RL and Juang, Tony Tong-Ying and Chen, Wen-Hao},
	date = {2020},
	journaltitle = {Measurement and Control},
	title = {Deep {Petri} nets of unsupervised and supervised learning},
	doi = {10.1177/0020294020923375},
	note = {\_eprint: https://doi.org/10.1177/0020294020923375},
	number = {7-8},
	pages = {1267--1277},
	volume = {53},
	abstract = {Artificial intelligence is one of the hottest research topics in computer science. In general, when it comes to the needs to perform deep learning, the most intuitive and unique implementation method is to use neural network. But there are two shortcomings in neural network. First, it is not easy to be understood. When encountering the needs for implementation, it often requires a lot of relevant research efforts to implement the neural network. Second, the structure is complex. When constructing a perfect learning structure, in order to achieve the fully defined connection between nodes, the overall structure becomes complicated. It is hard for developers to track the parameter changes inside. Therefore, the goal of this article is to provide a more streamlined method so as to perform deep learning. A modified high-level fuzzy Petri net, called deep Petri net, is used to perform deep learning, in an attempt to propose a simple and easy structure and to track parameter changes, with faster speed than the deep neural network. The experimental results have shown that the deep Petri net performs better than the deep neural network.},
	priority = {prio1}
}

@inproceedings{Michael16,
	author = {Defferrard, Micha\"{e}l and Bresson, Xavier and Vandergheynst, Pierre},
	title = {Convolutional neural networks on graphs with fast localized spectral filtering},
	year = {2016},
	isbn = {9781510838819},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	pages = {3844–3852},
	numpages = {9},
	location = {Barcelona, Spain},
	series = {NIPS'16}
}

@inproceedings{Morris19,
	author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
	title = {Weisfeiler and leman go neural: higher-order graph neural networks},
	year = {2019},
	isbn = {978-1-57735-809-1},
	publisher = {AAAI Press},
	url = {https://doi.org/10.1609/aaai.v33i01.33014602},
	doi = {10.1609/aaai.v33i01.33014602},
	abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
	booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
	articleno = {565},
	numpages = {8},
	location = {Honolulu, Hawaii, USA},
	series = {AAAI'19/IAAI'19/EAAI'19}
}

@PhdThesis{Petri1962,
	author = {Carl Adam Petri},
	date = {1962},
	institution = {Universität Hamburg},
	title = {Kommunikation mit Automaten},
	language = {ger}
}

@Article{Saleh2023,
	author = {Ali Saleh and Manuel Chiach\'{i}o and Juan Fern\'{a}ndez Salas and Athanasios Kolios},
	date = {2023-03},
	journaltitle = {Reliability Engineering {\&} System Safety},
	title = {Self-adaptive optimized maintenance of offshore wind turbines by intelligent Petri nets},
	doi = {https://doi.org/10.1016/j.ress.2022.109013},
	issn = {0951-8320},
	pages = {109013},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832022006287},
	volume = {231},
	keywords = {Petri net, Reinforcement learning, Q-learning, Offshore wind turbines, Condition-based maintenance},
	priority = {prio2},
	publisher = {Elsevier {BV}}
}

@Article{Shen2010,
	author = {Shen, Victor R. L. and Chang, Yue-Shan and Juang, Tony Tong-Ying},
	date = {2010-03},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	title = {Supervised and Unsupervised Learning by Using Petri Nets},
	doi = {10.1109/tsmca.2009.2038068},
	issn = {1558-2426},
	number = {2},
	pages = {363--375},
	volume = {40},
	priority = {prio2},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})}
}

@Article{Vahidipour2015,
	author = {Vahidipour, S. Mehdi and Meybodi, Mohammad Reza and Esnaashari, Mehdi},
	date = {2015-10},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
	title = {Learning Automata-Based Adaptive Petri Net and Its Application to Priority Assignment in Queuing Systems With Unknown Parameters},
	doi = {10.1109/tsmc.2015.2406764},
	issn = {2168-2216, 2168-2232},
	number = {10},
	pages = {1373--1384},
	url = {http://ieeexplore.ieee.org/document/7056543/},
	urldate = {2022-12-26},
	volume = {45},
	priority = {prio2},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})}
}

@Article{Wang2022a,
	author = {Junli Wang and Hongda Qi and Mingjian Guang and Chaobo Zhang and Chungang Yan and Changjun Jiang},
	date = {2022-12},
	journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
	title = {Net Learning},
	doi = {10.1109/tnnls.2021.3084902},
	issn = {2162-2388},
	number = {12},
	pages = {7380--7389},
	volume = {33},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)}
}

@inproceedings{You22,
	author = {You, Jiaxuan and Du, Tianyu and Leskovec, Jure},
	title = {ROLAND: Graph Learning Framework for Dynamic Graphs},
	year = {2022},
	isbn = {9781450393850},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3534678.3539300},
	doi = {10.1145/3534678.3539300},
	abstract = {Graph Neural Networks (GNNs) have been successfully applied to many real-world static graphs. However, the success of static graphs has not fully translated to dynamic graphs due to the limitations in model design, evaluation settings, and training strategies. Concretely, existing dynamic GNNs do not incorporate state-of-the-art designs from static GNNs, which limits their performance. Current evaluation settings for dynamic GNNs do not fully reflect the evolving nature of dynamic graphs. Finally, commonly used training methods for dynamic GNNs are not scalable. Here we propose ROLAND, an effective graph representation learning framework for real-world dynamic graphs. At its core, the ROLAND framework can help researchers easily repurpose any static GNN to dynamic graphs. Our insight is to view the node embeddings at different GNN layers as hierarchical node states and then recurrently update them over time. We then introduce a live-update evaluation setting for dynamic graphs that mimics real-world use cases, where GNNs are making predictions and being updated on a rolling basis. Finally, we propose a scalable and efficient training approach for dynamic GNNs via incremental training and meta-learning. We conduct experiments over eight different dynamic graph datasets on future link prediction tasks. Models built using the ROLAND framework achieve on average 62.7\% relative mean reciprocal rank (MRR) improvement over state-of-the-art baselines under the standard evaluation settings on three datasets. We find state-of-the-art baselines experience out-of-memory errors for larger datasets, while ROLAND can easily scale to dynamic graphs with 56 million edges. After re-implementing these baselines using the ROLAND training strategy, ROLAND models still achieve on average 15.5\% relative MRR improvement over the baselines.},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {2358–2366},
	numpages = {9},
	keywords = {dynamic graphs, graph neural networks, network analysis},
	location = {Washington DC, USA},
	series = {KDD '22}
}

@Book{german00:_perfor_analy_commun_system,
	url = {https://www.wiley.com/en-us/Performance+Analysis+of+Communication+Systems+%3A+Modeling+with+Non-Markovian+Stochastic+Petri+Nets-p-9780471492580},
	publisher = {Wiley},
	series = {Wiley Interscience Series In Systems And Optimization},
	year = {2000},
	title = {Performance Analysis of Communication Systems: Modeling with Non-Markovian Stochastic Petri Nets},
	author = {Reinhard German}
}

@inproceedings{hamilton17,
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	title = {Inductive representation learning on large graphs},
	year = {2017},
	isbn = {9781510860964},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	pages = {1025–1035},
	numpages = {11},
	location = {Long Beach, California, USA},
	series = {NIPS'17}
}

@Article{kipf17:_semi_super_class_graph_convol_networ,
	journal = {International Conference on Learning Representations},
	url = {https://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	year = {2017},
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	author = {Thomas N. Kipf and Max Welling}
}

@Article{mator20:_bottl_detec_cloud_comput_perfor_depen,
	url = {https://doi.org/10.1007/s10922-020-09562-9},
	doi = {10.1007/s10922-020-09562-9},
	pages = {1839--1871},
	volume = {28},
	year = {2020},
	date = {2020-10-01},
	journaltitle = {Journal of Network and Systems Management},
	title = {Bottleneck Detection in Cloud Computing Performance and Dependability: Sensitivity Rankings for Hierarchical Models},
	author = {Matos, Rubens and Dantas, Jamilson and Araujo, Eltton and Maciel, Paulo}
}

@Book{peterson81:_petri_net_theor_and_the,
	publisher = {Prentice Hall},
	year = {1981},
	date = {1981-02-01},
	title = {PETRI NET THEORY AND THE MODELING OF SYSTEMS},
	author = {James L. Peterson}
}

@Article{you20:_desig_space_graph_neural_networ,
	journal = {NeurIPS},
	year = {2020},
	eventtitleaddon = {NeurIPS},
	title = {Design Space for Graph Neural Networks},
	author = {Jiaxuan You and Rex Ying and Jure Leskovec}
}

@Comment{
Local Variables:
bibtex-dialect: biblatex
End:
}

